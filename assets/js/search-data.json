{
  
    
        "post0": {
            "title": "Deep learning with standard Python",
            "content": "Here, we will be traing a deep learning model with only Python. The only 3rd party libary I will be using is matplotlib to show images. It is obviously not meant to train a state of the art model or a production model. Instead, we will explore what it means to train a model using MNIST handwritten dataset. When I was reading a blog post from Sylvain Gugger called, &quot;A simple neural net in numpy,&quot; I found it interesting and wanted to try to write my version in standard Python. . There are many libraries used for deep learning, such as Tensorflow and Keras, Pytorch, Scikit-Learn, numpy, and others. However, in order to understand basic concept of deep learning, we do no need to use any of these tools. . Here are Python libraries I will be using. Other than matplotlib, all libraries are here: . import gzip # Converting zip into python objects import matplotlib.pyplot as plt # Library for showing images import random # Initializing random weights import statistics # Getting mean or stdev import math # Exponential function import operator as op # just algebraic operators such as +, -, *, / import time # Timing for performance from functools import reduce . import numpy as np . Utilities . This section can be skipped as you can understand how deep learning works without understanding how these utilities work. You can consider these as built-in functions or machine language blobs. Simply fold the heading and run it. . If you decided to dig into lower level of deep learning, let&#39;s begin. Because we are only using python lists, we need to build utilities or tools we need in order to train our model. With other fancy libraries, these are built-in. However, because we are building up from scratch, we start with those basic utilities that we will use later on. How these are implemented specifically are not important at all for our purpose of understanding how deep learning works. As long as you get the general idea of what these functions do, that is good enough. However, please do not just run those cells without thinking about what they will return in advance if you want to understand more. Give yourself couple seconds to think and come up with output in your head. This will change your level of understanding. . With those low level functions, we will build up abstraction layers. First, I start with shape, which returns a tuple of shape of a matrix or a list. . def shape(matrix) -&gt; tuple: &quot;&quot;&quot; Get a shape of a matrix &quot;&quot;&quot; def loop(mat, result): if not isinstance(mat, list): return result else: return loop(mat[0], result + (len(mat),)) return loop(matrix, tuple()) . shape([1, 2]) . (2,) . shape([[1, 2, 3], [4, 5, 6]]) . (2, 3) . shape([[[1, 2], [3, 4]]]) . (1, 2, 2) . It would be better if we can make matrices easier instead of making them by hand. . def lst_nums(shape:tuple, num:int=1) -&gt; list: &quot;&quot;&quot; Return a list of shape filled with num. Default value for num is 1 &quot;&quot;&quot; if isinstance(shape, tuple): x, y = shape return [[num]*y for _ in range(x)] else: x = shape return [num]*x . lst_nums(1, 1) . [1] . hund_1s = lst_nums((10, 10), 1) len(hund_1s), len(hund_1s[0]) . (10, 10) . hund_1s . [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] . Here is a transpose function, which transposes a matrix. If you are not familiar with what it does, here is a wikipedia page with an animation. . def transpose (mat) -&gt; list: &quot;Transpose the matrix&quot; return [[m[i] for m in mat] for i in range(len(mat[0]))] . mat1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] . mat1 . [[1, 2, 3], [4, 5, 6], [7, 8, 9]] . transpose(mat1) . [[1, 4, 7], [2, 5, 8], [3, 6, 9]] . map_mat calls a fn (function) to a mat1 if there is only one matrix. If there are two matrices, it uses individual elements from both mat1 and mat2 as arguments for the fn. This is a long and complicated function, and I don&#39;t think I did a good job of writing it. It might be better to divide them by smaller functions, but I&#39;m not sure. How would you improve it? . def map_mat(fn, mat1, mat2=None) -&gt; list: &quot;&quot;&quot; If there is only one matrix, call the function on the matrix. If map_mat is called with two matrices, call the function with individual elements from mat1 and mat2 respectfully. This function can handle broadcasting when mat2 is a vector. &quot;&quot;&quot; if mat2 == None: return [list(map(fn, mat1[i])) for i in range(len(mat1))] mat = [] try: m1r,m1c = shape(mat1) except ValueError: m1r = shape(mat1)[0] m1c = 0 try: m2r,m2c = shape(mat2) except ValueError: m2r = shape(mat2)[0] m2c = 0 if m1c == m2c == 0: # Two 1D vectors return list(map(fn, mat1, mat2)) elif (m1r, m1c) == (m2r, m2c): # two matrixs with same sizes return [[fn(x,y) for x,y in zip(mat1[i], mat2[i])] for i in range(len(mat1))] elif m1c == m2r and m2c==0: # shape of (a, b), (b,) for i in range(m1r): mat.append([fn(x,y) for x,y in zip(mat1[i],mat2)]) return mat elif m1r == m2r and m2c == 0: # shape of (a, b), (a,) for i in range(m1r): mat.append([fn(m, mat2[i]) for m in mat1[i]]) return mat else: raise Exception(&quot;map_mat error&quot;) . hund_2s = lst_nums((10, 10), 2) . hund_3s = map_mat(lambda x, y: x+y, hund_1s, hund_2s) hund_3s . [[3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]] . map_mat(op.mul, hund_3s, hund_3s) . [[9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]] . map_mat(lambda x: x + 1, mat1) . [[2, 3, 4], [5, 6, 7], [8, 9, 10]] . Next, we have reshape function, which reshapes a matrix into new_shape. . def reshape(matrix, new_shape) -&gt; list: &quot;&quot;&quot; If matrix can be reshaped into new_shape, then return a new matrix with a respective shape. &quot;&quot;&quot; old_shape = shape(matrix) elem_nums = mul(old_shape) if old_shape == new_shape: return matrix elif not elem_nums == mul(new_shape): raise Exception(&quot;Wrong shape!&quot;) else: return shaping(flatten(matrix), new_shape, elem_nums) . def mul(lst: list) -&gt; int: &quot;&quot;&quot; Return a result of all numbers multiplied. Like sum, but multiplying. &quot;&quot;&quot; return reduce(op.mul, lst, 1) . def shaping(flat, new_shape, elem_nums): &quot;&quot;&quot; Actually shaping flat array into new shape. &quot;&quot;&quot; new_shape_len = len(new_shape) if new_shape_len == 1 or new_shape_len == 0: return result div = elem_nums // new_shape[0] # div = 50 result = [flat[(i * div):((i+1) * div)] for i in range(new_shape[0])] if new_shape_len == 2: return result else: return [shaping(result[i], new_shape[1:], div) for i in range(new_shape[0])] . def flatten(matrix): &quot;&quot;&quot; Flatten a matrix into a 1 dimensional list. &quot;&quot;&quot; result = [] for i in range(len(matrix)): if isinstance(matrix[i], list): result.extend(flatten(matrix[i])) else: result.append(matrix[i]) return result . Testing new tools . shape(reshape(hund_1s, (100, 1))), shape(reshape(hund_1s, (1, 100))) . ((100, 1), (1, 100)) . shape(reshape(hund_1s, (2, 5, 10))) . (2, 5, 10) . mat3 = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] mat3, shape(mat3) . ([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], (2, 2, 2)) . shape(reshape(mat3, (4, 2))), reshape(mat3, (4, 2)) . ((4, 2), [[1, 2], [3, 4], [5, 6], [7, 8]]) . Collecting Data . With those utilities, we can get started on getting our hands dirty with deep learning. . First, we need data if we want to do some training. We are using MNIST dataset, which contains handwritten digits, from Yann Lecun website. The dataset has 60,000 training images and 10,000 testing images. When we look at amazing things computers were trained to do, such as self driving cars and speech recognition, there were actually a lot of data fed into them for training. Although we tend to only give credits to those who came up with state of the art models, nothing could have been possible without those who prepared for data. . !wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz !wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz !wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz !wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz . --2021-09-03 01:35:44-- http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Resolving yann.lecun.com (yann.lecun.com)... 172.67.171.76, 104.21.29.36, 2606:4700:3034::6815:1d24, ... Connecting to yann.lecun.com (yann.lecun.com)|172.67.171.76|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 9912422 (9.5M) [application/x-gzip] Saving to: ‘train-images-idx3-ubyte.gz’ train-images-idx3-u 100%[===================&gt;] 9.45M 37.6MB/s in 0.3s 2021-09-03 01:35:44 (37.6 MB/s) - ‘train-images-idx3-ubyte.gz’ saved [9912422/9912422] --2021-09-03 01:35:44-- http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Resolving yann.lecun.com (yann.lecun.com)... 172.67.171.76, 104.21.29.36, 2606:4700:3034::6815:1d24, ... Connecting to yann.lecun.com (yann.lecun.com)|172.67.171.76|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 28881 (28K) [application/x-gzip] Saving to: ‘train-labels-idx1-ubyte.gz’ train-labels-idx1-u 100%[===================&gt;] 28.20K --.-KB/s in 0.003s 2021-09-03 01:35:44 (10.7 MB/s) - ‘train-labels-idx1-ubyte.gz’ saved [28881/28881] --2021-09-03 01:35:44-- http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Resolving yann.lecun.com (yann.lecun.com)... 172.67.171.76, 104.21.29.36, 2606:4700:3034::6815:1d24, ... Connecting to yann.lecun.com (yann.lecun.com)|172.67.171.76|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 1648877 (1.6M) [application/x-gzip] Saving to: ‘t10k-images-idx3-ubyte.gz’ t10k-images-idx3-ub 100%[===================&gt;] 1.57M 9.67MB/s in 0.2s 2021-09-03 01:35:44 (9.67 MB/s) - ‘t10k-images-idx3-ubyte.gz’ saved [1648877/1648877] --2021-09-03 01:35:44-- http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Resolving yann.lecun.com (yann.lecun.com)... 172.67.171.76, 104.21.29.36, 2606:4700:3034::6815:1d24, ... Connecting to yann.lecun.com (yann.lecun.com)|172.67.171.76|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 4542 (4.4K) [application/x-gzip] Saving to: ‘t10k-labels-idx1-ubyte.gz’ t10k-labels-idx1-ub 100%[===================&gt;] 4.44K --.-KB/s in 0s 2021-09-03 01:35:45 (334 MB/s) - ‘t10k-labels-idx1-ubyte.gz’ saved [4542/4542] . !ls . sample_data train-images-idx3-ubyte.gz t10k-images-idx3-ubyte.gz train-labels-idx1-ubyte.gz t10k-labels-idx1-ubyte.gz . Now, I am making a directory for all the data and putting data inside. . !mkdir data !mv train-images-idx3-ubyte.gz data/ !mv train-labels-idx1-ubyte.gz data/ !mv t10k-images-idx3-ubyte.gz data/ !mv t10k-labels-idx1-ubyte.gz data/ !ls data/ . t10k-images-idx3-ubyte.gz train-images-idx3-ubyte.gz t10k-labels-idx1-ubyte.gz train-labels-idx1-ubyte.gz . Now, we have files we need. With py_mnist_images and py_mnist_labels, we convert these files into lists of images. I got those functions from here, which originally returned numpy arrays, but I changed them to return lists. After calling those functions, we call reshape function to make them shapes of images, which are 28 by 28 pixels. . def py_mnist_images(fname:str, pct:1) -&gt; list: &quot;&quot;&quot; Convert zip files into lists of images. Only returning pct percent of data. &quot;&quot;&quot; with gzip.open(&#39;data/&#39;+fname, &#39;r&#39;) as f: # first 4 bytes is a magic number magic_number = int.from_bytes(f.read(4), &#39;big&#39;) # second 4 bytes is the number of images image_count = int.from_bytes(f.read(4), &#39;big&#39;) # image_count = int(image_count * percent) # third 4 bytes is the row count row_count = int.from_bytes(f.read(4), &#39;big&#39;) # fourth 4 bytes is the column count column_count = int.from_bytes(f.read(4), &#39;big&#39;) # rest is the image pixel data, each pixel is stored as an unsigned byte # pixel values are 0 to 255 image_data = f.read() images = reshape(list(image_data), (image_count, column_count, row_count)) return images[:int(image_count * pct)] # return reshape(images, (image_count, column_count, row_count)) def py_mnist_labels(fname:str, pct:1) -&gt; list: &quot;&quot;&quot; Convert zip files into lists of labels. Only returning pct percent of data. &quot;&quot;&quot; with gzip.open(&#39;data/&#39;+fname, &#39;r&#39;) as f: # first 4 bytes is a magic number magic_number = int.from_bytes(f.read(4), &#39;big&#39;) # second 4 bytes is the number of labels label_count = int.from_bytes(f.read(4), &#39;big&#39;) # rest is the label data, each label is stored as unsigned byte # label values are 0 to 9 label_data = f.read() labels = list(label_data) return labels[:int(label_count * pct)] . With py_mnist_images, we get lists of images. We call these matrices. When an array has 1 dimension, it is a vector, and an array 2 or more dimensions is called matrix. . Let&#39;s us only 1% of the data because python is slow. . train_imgs = py_mnist_images(&#39;train-images-idx3-ubyte.gz&#39;, pct=0.01) train_labels = py_mnist_labels(&#39;train-labels-idx1-ubyte.gz&#39;, pct=0.01) test_imgs = py_mnist_images(&#39;t10k-images-idx3-ubyte.gz&#39;, pct=0.01) test_labels = py_mnist_labels(&#39;t10k-labels-idx1-ubyte.gz&#39;, pct=0.01) . Let&#39;s take a look at those data before we move on. We want to make sure we are working with correct data. Using plt.imshow, we can take a look at each image. . type(train_imgs), type(train_imgs[0][0][0]) . (list, int) . shape(train_imgs), shape(train_labels), shape(test_imgs), shape(test_labels) . ((600, 28, 28), (600,), (100, 28, 28), (100,)) . Here are pictures with labels. . plt.imshow(train_imgs[0], cmap=&#39;gray&#39;); train_labels[0] . 5 . plt.imshow(train_imgs[1], cmap=&#39;gray&#39;); train_labels[1] . 0 . plt.imshow(train_imgs[2], cmap=&#39;gray&#39;); train_labels[2] . 4 . Now that we have some tools to work with, we can prepare our data for training. First, we will reshape our data. We are combining x and y axis into a vector (1-dimensional array) so that one array equals to one image. Then, we normalize our data by dividing them by 255 because the highest value is 255. . train_imgs = reshape(train_imgs, (600, 28 * 28)) test_imgs = reshape(test_imgs, (100, 28 * 28)) shape(train_imgs), shape(test_imgs) . ((600, 784), (100, 784)) . train_imgs = map_mat(lambda x: x / 255, train_imgs) test_imgs = map_mat(lambda x: x / 255, test_imgs) . train_imgs[0][400:450] . [0.0, 0.0, 0.0, 0.0, 0.0, 0.3176470588235294, 0.9411764705882353, 0.9921568627450981, 0.9921568627450981, 0.4666666666666667, 0.09803921568627451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17647058823529413, 0.7294117647058823, 0.9921568627450981, 0.9921568627450981, 0.5882352941176471, 0.10588235294117647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] . Initializing Weights . data + weight &amp; bias = model . Another ingredient for deep learning is a set of weights. . def lst_random(shape, init_parms=False): &quot;return a list of randoms and if init_parms is True, initialize parameters using Kaiming init.&quot; x, y = shape res = lst_nums(shape, 0) for i in range(x): for j in range(y): res[i][j] = random.normalvariate(0,1) if init_parms: res[i][j] *= math.sqrt(2/x) return res . rand_mat = lst_random((10,10)) shape(rand_mat) . (10, 10) . Using Kaiming init. With Kaiming init, we get a head start compared to using just random numbers. . sample = lst_random((200, 100), True) # x = map_mat(lambda x: x*0.1, x) # statistics.stdev(x[0]) . Checking whether the initialization works. Standard deviation should equal to sqrt(2/n_in), and mean should be 0. And this works. With this initialization, we can train deeper layers. For more information, paper is here. . def check_dist(x): for i in range(len(x)//10): print(statistics.stdev(x[i]), statistics.mean(x[i])) . math.sqrt(2/200) . 0.1 . check_dist(sample) . 0.09297750982483842 -0.005551492975462343 0.09377392014470953 -0.0004644530715541377 0.0880266808855306 0.0014302874910074933 0.10476777083224814 0.006420732596219193 0.11401894394433278 0.0026472387089674365 0.09504169857687136 -0.016832640036345214 0.10115404723103125 0.008298330317628617 0.10003722020964131 -0.001963013655344632 0.10840916095212666 -0.003917221072107054 0.09475181552257386 0.010960612697832342 0.10749452903124168 -0.00043170903663133643 0.11426827714443664 -0.01592956027184722 0.09805584337087062 0.006115694202963703 0.09530437918322529 0.0010778647613035926 0.10786920778882839 -4.886472580181381e-05 0.09951682764549248 0.004193264024171723 0.10069742267004245 -0.0041036793564703114 0.12237299727220302 -0.008247944715812132 0.10771116381117542 0.010819112201489189 0.09027209249627051 -0.010052280483938152 . . Matrix multiplication . Now that our data is ready, it is time to look at matrix multiplication, which is the most frequently used operation in deep learning. . x = lst_random((200,100)) . x[1][:5] . [0.9347526378113412, -0.1316092325249455, -1.7432261605942205, -0.5668829848642469, -0.22568080371154053] . If shape is (2,3): [[1,1,1], [1,1,1]] Also, if matrix multiplication between (2, 3) and (3, 4) should be (2, 4) . def py_matmul1(a,b): &quot;Needs some speed ups&quot; ar,ac = len(a),len(a[0]) br,bc = len(b),len(b[0]) assert ac == br, f&#39;Size of ar ({ac}) does not match br ({br}).&#39; c = lst_nums((ar, bc), 0) for i in range(ar): for j in range(bc): for z in range(ac): c[i][j] += a[i][z] * b[z][j] return c . m1 = [[1,2],[3,4]] m2 = [[0,3],[2,5]] m5 = [[1,2,3,4],[5,6]] . py_matmul1(m1,m2) . [[10, 13], [22, 29]] . ml1 = lst_random((784, 100)) ml2 = lst_random((100, 10)) . It works, but it is slow. We can make it faster by getting rid of for loop. . . def col_mat (mat:list, col:int) -&gt; list: &quot;Get a column of a matrix.&quot; return [m[col] for m in mat] . def py_matmul2(a,b): &quot;Use sum function&quot; ar,ac = len(a),len(a[0]) br,bc = len(b),len(b[0]) assert ac == br, f&#39;Size of ar ({ac}) does not match br ({br}).&#39; c = lst_nums((ar, bc), 0) for i in range(ar): for j in range(bc): c[i][j] = sum(map_mat(op.mul, a[i], col_mat(b,j))) return c . Using two for loops is faster than using three. . . def py_matmul3(a, b): ar,ac = len(a),len(a[0]) br,bc = len(b),len(b[0]) assert ac == br, f&#39;Size of ar ({ac}) does not match br ({br}).&#39; c = lst_nums((ar, bc), 0) for i in range(ar): c[i] = [sum(map_mat(op.mul, a[i], col_mat(b,j))) for j in range(bc)] return c . . . . Even with reducing it to one loop, we did not really gain much speed. After using prun, we can see that elementwise is using a lot of time. We can probably get away without using elemtwise to achieve matrix multiplication. . def py_matmul4(a, b): ar,ac = len(a),len(a[0]) br,bc = len(b),len(b[0]) assert ac == br, f&#39;Size of ar ({ac}) does not match br ({br}).&#39; c = lst_nums((ar, bc), 0) t = transpose(b) for i in range(ar): c[i] = [sum(map(lambda x: x[0] * x[1], zip(a[i], (t[j])))) for j in range(bc)] return c . . Without elementwise, we gained some speed compared to other versions. . . . I am still not satisfied with the result yet. I am sure we can do better. Let&#39;s get some help from itertools. . Default sum takes the longest time to execute now, but it is faster option we have, compared to using for loop or reduce function. . def py_matmul(a, b): ar,ac = len(a),len(a[0]) br,bc = len(b),len(b[0]) assert ac == br, f&#39;Size of ar ({ac}) does not match br ({br}).&#39; c = lst_nums((ar, bc), 0) t = transpose(b) for i in range(ar): c[i] = [sum(map(op.mul, a[i], t[j])) for j in range(bc)] return c . py_matmul(m1, m2) . [[10, 13], [22, 29]] . Model . First, we will make a model with classes first. . m1_np = np.array(m1) m2_np = np.array(m2) . class Relu(): def forward(self, x): self.old_x = x.copy() res = map_mat(lambda y: 0 if y &lt; 0 else y, x) return res def backward(self, grad): res = map_mat(lambda x, g: g if x &gt; 0 else 0, self.old_x, grad) return res . class Relu_np(): def forward(self, x): self.old_x = np.copy(x) return np.clip(x,0,None) def backward(self, grad): return np.where(self.old_x&gt;0,grad,0) . relu = Relu() relu_np = Relu_np() relu.forward(m1), relu_np.forward(m1) . ([[1, 2], [3, 4]], array([[1, 2], [3, 4]])) . relu.backward(m2), relu_np.backward(m2_np) . ([[2, 3], [4, 5]], array([[2, 3], [4, 5]])) . class Softmax(): def forward(self, inp): mat = map_mat(math.exp, inp) self.old_y = [] for i in range(len(mat)): s = sum(mat[i]) self.old_y.append([x/s for x in mat[i]]) return self.old_y def backward(self, grad): res = map_mat(op.mul, self.old_y, grad) res = [sum(res[i]) for i in range(len(self.old_y))] # shape is (64,) # res = reshape(res, (2, 1)) # print(f&#39;Softmax: {res}&#39;) # res = map_mat(op.sub, grad, res) res = [list(map(lambda x: x - res[i], grad[i])) for i in range(len(grad))] # print(f&#39;Softmax: {res}&#39;) return map_mat(op.mul, self.old_y, res) . class Softmax_np(): def forward(self,x): self.old_y = np.exp(x) / np.exp(x).sum(axis=1) [:,None] return self.old_y def backward(self,grad): res = (grad * self.old_y).sum(axis=1)[:,None] print(f&#39;Softmax_np: {res}&#39;) res = grad - res print(f&#39;Softmax_np: {res}&#39;) # return self.old_y * (grad - (grad * self.old_y).sum(axis=1)[:,None]) return self.old_y * res . m1_np.sum(axis=1)[:,None] . array([[3], [7]]) . sm = Softmax() sm_np = Softmax_np() sm.forward(m1), sm_np.forward(m1) . ([[0.2689414213699951, 0.7310585786300049], [0.2689414213699951, 0.7310585786300048]], array([[0.26894142, 0.73105858], [0.26894142, 0.73105858]])) . m2_np . array([[0, 3], [2, 5]]) . m2_np - [[2], [1]] . array([[-2, 1], [ 1, 4]]) . m2_np - [[2.19317574],[4.19317574]] . array([[-2.19317574, 0.80682426], [-2.19317574, 0.80682426]]) . sm.backward(m2), sm_np.backward(m2_np) . Softmax: [2.193175735890015, 4.193175735890014] Softmax: [[-2.193175735890015, 0.8068242641099852], [-2.193175735890014, 0.8068242641099861]] Softmax_np: [[2.19317574] [4.19317574]] Softmax_np: [[-2.19317574 0.80682426] [-2.19317574 0.80682426]] . ([[-0.5898357997244456, 0.5898357997244454], [-0.5898357997244453, 0.589835799724446]], array([[-0.5898358, 0.5898358], [-0.5898358, 0.5898358]])) . def softmax_b(old_y, grad): # shape of old_y is (64, 10), and grad is (64,) res = map_mat(op.mul, old_y, grad) res = [sum(res[i]) for i in range(len(old_y))] # shape is (64,) return map_mat(op.mul, old_y, map_mat(op.sub, grad, res)) . softmax_b([[0.2689414213699951, 0.7310585786300049],[0.2689414213699951, 0.7310585786300048]], m2) . [[-0.19661193324148193, -1.2655052240185276], [0.34127090949850825, 0.1966119332414822]] . class CrossEntropy(): def forward(self, inp, targ): mat = map_mat(lambda x: x if x&gt;1e-8 else 1e-8, inp) self.old_x = mat.copy() self.old_y = targ res = [] for i in range(len(mat)): for j in range(len(targ[0])): if targ[i][j] == 1: res.append(-math.log(mat[i][j])) return res def backward(self): mat = map_mat(lambda x: x if x&gt;1e-8 else 1e-8, self.old_x) # mat = self.old_x res = lst_nums(shape(self.old_x), num=0.) for i in range(len(mat)): for j in range(len(self.old_y[0])): if self.old_y[i][j] == 1: res[i][j] = (-1/(mat[i][j])) return res . class CrossEntropy_np(): def forward(self,x,y): self.old_x = x.clip(min=1e-8,max=None) self.old_y = y return (np.where(y==1,-np.log(self.old_x), 0)).sum(axis=1) def backward(self): return np.where(self.old_y==1,-1/self.old_x, 0) . ce = CrossEntropy() ce_np = CrossEntropy_np() ce.forward(m1, [[0, 1], [1, 0]]), ce_np.forward(m1_np, np.array([[0, 1], [1, 0]])) . ([-0.6931471805599453, -1.0986122886681098], array([-0.69314718, -1.09861229])) . ce.backward(), ce_np.backward() . ([[0.0, -0.5], [-0.3333333333333333, 0.0]], array([[ 0. , -0.5 ], [-0.33333333, 0. ]])) . class Linear(): def __init__(self, n_in, n_out): self.weights = lst_random((n_in, n_out), True) # self.weights = [[1, 2], [3, 4]] self.biases = lst_nums((n_out), num=0) def forward(self, inp): self.old_x = inp return map_mat(lambda x, y: x + y, py_matmul(inp, self.weights), self.biases) def backward(self, grad): self.grad_b = mean_0(grad) self.grad_w = py_matmul(transpose(self.old_x), grad) out = py_matmul(grad, transpose(self.weights)) return out . class Linear_np(): def __init__(self,n_in,n_out): self.weights = np.random.randn(n_in,n_out) * np.sqrt(2/n_in) # self.weights = np.array([[1, 2], [3, 4]]) self.biases = np.zeros(n_out) def forward(self, x): self.old_x = x return np.dot(x,self.weights) + self.biases def backward(self,grad): self.grad_b = grad.mean(axis=0) self.grad_w = (np.matmul(self.old_x[:,:,None],grad[:,None,:])).mean(axis=0) return np.dot(grad,self.weights.transpose()) . l = Linear(2,2) l_np = Linear_np(2,2) l.forward(m1), l_np.forward(m1_np) . ([[7, 10], [15, 22]], array([[ 7., 10.], [15., 22.]])) . l.backward(m2), l_np.backward(m2_np) . ([[8, 18], [14, 32]], array([[ 8, 18], [14, 32]])) . def mean_0 (matrix): &quot;Find a mean in matrix over 0 axis&quot; return [statistics.mean([m[i] for m in matrix]) for i in range(len(matrix[0]))] . class Model(): def __init__(self, layers, cost): self.layers = layers self.cost = cost def forward(self,x): for layer in self.layers: x = layer.forward(x) return x def make_preds(self, x): outputs = self.forward(x) # (64, 10) preds = [outputs[i].index(max(outputs[i])) for i in range(len(outputs))] return preds def loss(self,x,y): return self.cost.forward(self.forward(x),y) def backward(self): grad = self.cost.backward() for i in range(len(self.layers)-1,-1,-1): # print(f&#39;grad for one before {self.layers[i]} is {shape(grad)}&#39;) grad = self.layers[i].backward(grad) # print(f&#39;first gradient is {grad[0]}&#39;) . class Model_np(): def __init__(self, layers, cost): self.layers = layers self.cost = cost def forward(self,x): for layer in self.layers: x = layer.forward(x) return x def loss(self,x,y): return self.cost.forward(self.forward(x),y) def backward(self): grad = self.cost.backward() for i in range(len(self.layers)-1,-1,-1): grad = self.layers[i].backward(grad) . def load_minibatches(trn, targets, bs=64): data = [] for i in range((len(trn) // bs) - 1): targs = lst_nums((bs, 10), 0) targets_mb = targets[(i*bs) : ((i+1)*bs)] for z in range(bs): targs[z][targets_mb[z]] = 1. data.append((trn[:bs],targs)) return data . dataset = load_minibatches(train_imgs, train_labels) shape(dataset) . (8,) . def train(model, lr ,nb_epoch, data, testset): for epoch in range(nb_epoch): running_loss = 0. num_inputs = 0 for mini_batch in data: corrects = 0 inputs, targets = mini_batch test_inp, test_targs = testset num_inputs += len(inputs) #Forward pass + compute loss # print(shape(model.loss(inputs,targets))) preds = model.make_preds(test_inp) for i in range(len(preds)): if test_targs[i] == preds[i]: corrects += 1 # corrects = [test_targs[i] == preds[i] for i in range(len(preds))] # print(corrects) running_loss += sum(model.loss(inputs,targets)) #Back propagation model.backward() #Update the parameters for layer in model.layers: if type(layer) == Linear: weight_diff = [list(map(lambda x: x * lr, layer.grad_w[i])) for i in range(len(layer.grad_w))] # print(f&#39;weight_diff: {shape(weight_diff)}, weights: {shape(layer.weights)}, grad_w: {shape(layer.grad_w)}&#39;) layer.weights = map_mat(op.sub, layer.weights, weight_diff) # layer.weights -= lr * layer.grad_w bias_diff = list(map(lambda x: x * lr, layer.grad_b)) layer.biases = map_mat(op.sub, layer.biases, bias_diff) # layer.biases -= lr * layer.grad_b print(f&#39;loss = {running_loss/num_inputs}, Accuracy = {corrects*100 / len(preds)}&#39;) print(f&#39;Epoch {epoch+1}/{nb_epoch}: loss = {running_loss/num_inputs}, Accuracy = {corrects*100 / len(preds)}&#39;) . net = Model([Linear(784,60), Relu(), Linear(60,10), Softmax()], CrossEntropy()) . train(net, 0.001, 10, dataset, (test_imgs, test_labels)) . loss = 2.430396220804606, Accuracy = 3.0 loss = 2.3679090179772224, Accuracy = 6.0 loss = 2.3763657151368887, Accuracy = 10.0 loss = 2.3531926701556354, Accuracy = 11.0 loss = 2.349502305412416, Accuracy = 12.0 loss = 2.3445673427045297, Accuracy = 12.0 loss = 2.343980645391663, Accuracy = 15.0 loss = 2.346984085706289, Accuracy = 15.0 Epoch 1/10: loss = 2.346984085706289, Accuracy = 15.0 loss = 2.2746041440900395, Accuracy = 14.0 loss = 2.248629992527297, Accuracy = 19.0 loss = 2.2742998351734496, Accuracy = 18.0 loss = 2.2635984830527427, Accuracy = 20.0 loss = 2.269421527953034, Accuracy = 19.0 loss = 2.2682471349208932, Accuracy = 22.0 loss = 2.2695385416261575, Accuracy = 21.0 loss = 2.2765253828171748, Accuracy = 22.0 Epoch 2/10: loss = 2.2765253828171748, Accuracy = 22.0 loss = 2.2108253149796955, Accuracy = 18.0 loss = 2.2019492572567123, Accuracy = 23.0 loss = 2.229776947823224, Accuracy = 22.0 loss = 2.223779633073442, Accuracy = 22.0 loss = 2.230734583312011, Accuracy = 20.0 loss = 2.2300639372993736, Accuracy = 24.0 loss = 2.230995460806782, Accuracy = 22.0 loss = 2.239133788677141, Accuracy = 21.0 Epoch 3/10: loss = 2.239133788677141, Accuracy = 21.0 loss = 2.1637874172666844, Accuracy = 20.0 loss = 2.1642453523489578, Accuracy = 24.0 loss = 2.1968603955378083, Accuracy = 24.0 loss = 2.193088499577704, Accuracy = 25.0 loss = 2.200949129451277, Accuracy = 24.0 loss = 2.2003248619602616, Accuracy = 25.0 loss = 2.200321654021033, Accuracy = 22.0 loss = 2.2092133211426677, Accuracy = 20.0 Epoch 4/10: loss = 2.2092133211426677, Accuracy = 20.0 loss = 2.128706200245396, Accuracy = 23.0 loss = 2.1352600440774294, Accuracy = 28.0 loss = 2.1707317564589226, Accuracy = 31.0 loss = 2.1675811758204415, Accuracy = 28.0 loss = 2.1761849824470407, Accuracy = 26.0 loss = 2.1755268977331306, Accuracy = 27.0 loss = 2.1747669587916265, Accuracy = 26.0 loss = 2.184248530612279, Accuracy = 22.0 Epoch 5/10: loss = 2.184248530612279, Accuracy = 22.0 loss = 2.096914805709899, Accuracy = 24.0 loss = 2.110028199899635, Accuracy = 29.0 loss = 2.1479599558252267, Accuracy = 31.0 loss = 2.145716811921206, Accuracy = 27.0 loss = 2.154453332101832, Accuracy = 28.0 loss = 2.153719837622655, Accuracy = 26.0 loss = 2.1524941265283806, Accuracy = 25.0 loss = 2.1624860124592624, Accuracy = 25.0 Epoch 6/10: loss = 2.1624860124592624, Accuracy = 25.0 loss = 2.0684100172010513, Accuracy = 26.0 loss = 2.088526361674454, Accuracy = 29.0 loss = 2.1280192693953492, Accuracy = 31.0 loss = 2.1266833769379696, Accuracy = 30.0 loss = 2.1362161133375284, Accuracy = 29.0 loss = 2.135729712673618, Accuracy = 27.0 loss = 2.133463990921976, Accuracy = 26.0 loss = 2.1441232630261924, Accuracy = 25.0 Epoch 7/10: loss = 2.1441232630261924, Accuracy = 25.0 loss = 2.042474705719687, Accuracy = 25.0 loss = 2.0686802916276763, Accuracy = 29.0 loss = 2.1093363533552925, Accuracy = 32.0 loss = 2.108749984227101, Accuracy = 30.0 loss = 2.1185467644183076, Accuracy = 29.0 loss = 2.117765112996861, Accuracy = 26.0 loss = 2.114600953171107, Accuracy = 25.0 loss = 2.1260570742199807, Accuracy = 26.0 Epoch 8/10: loss = 2.1260570742199807, Accuracy = 26.0 loss = 2.0181921974789097, Accuracy = 26.0 loss = 2.050477350651146, Accuracy = 31.0 loss = 2.0928595831545276, Accuracy = 33.0 loss = 2.09286273096861, Accuracy = 30.0 loss = 2.1028418018348622, Accuracy = 30.0 loss = 2.102337482298093, Accuracy = 27.0 loss = 2.0981297230113887, Accuracy = 25.0 loss = 2.1104566620707526, Accuracy = 26.0 Epoch 9/10: loss = 2.1104566620707526, Accuracy = 26.0 loss = 1.9949452526057696, Accuracy = 26.0 loss = 2.0332104830241526, Accuracy = 33.0 loss = 2.0771902880722846, Accuracy = 33.0 loss = 2.077378844823086, Accuracy = 30.0 loss = 2.0878159166580046, Accuracy = 30.0 loss = 2.087428056140829, Accuracy = 27.0 loss = 2.082140150809015, Accuracy = 26.0 loss = 2.0952495464836733, Accuracy = 27.0 Epoch 10/10: loss = 2.0952495464836733, Accuracy = 27.0 . net.layers[0].weights[:10] . net.make_preds(test_imgs)[:15] . [9, 9, 1, 0, 4, 1, 9, 9, 1, 7, 8, 6, 7, 6, 1] . test_labels[:15] . [7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1] . def validate(imgs, labels): corrects = 0 for i in range(len(imgs)): outputs = net(imgs[i]) preds = outputs.index(max(outputs)) print(outputs, preds) if preds == labels[i]: corrects += 1 print(f&#39;Accuracy: {100.*corrects/len(imgs)}&#39;) . validate(test_imgs, test_labels) . testset = tuple(test_imgs, test_labels) shape(testset) . Data Loader . Now, we will take mini bathces of data with batch size and train. . xb = py_imgs[:6400] yb = lst_nums((6400, 10), 0) yb_vals = py_train_labels[:4] for i in range(4): yb[i][yb_vals[i]] = 1 data = [(xb, yb)] shape(xb), shape(yb), shape(data) . ((6400, 784), (6400, 10), (1,)) . 64*100 . 6400 . shape(dset[0]) . () . def prep_data(x=train_imgs, y=train_labels): size = len(train_labels) yb = lst_nums((size, 10), 0) yb_vals = y[:size] for i in range(size): yb[i][yb_vals[i]] = 1 return x, yb . x, y = prep_data(600) shape(x), shape(y) . ((25600, 784), (25600, 10)) . x, y = prep_data() shape(x), shape(y) . ((600, 784), (600, 10)) . def forward_and_backward(inp, targ, w1, b1, w2, b2): l1 = linear(inp,w1,b1) l2 = map_mat(relu, l1) # l = linear(l, w2, b2) # l = softmax(l) # sm = softmax sm_old_y = linear(l2,w2,b2) # we don&#39;t actually need the loss in backward! # cel = Cross Entropy Loss cel_old_x = softmax(sm_old_y) cel_old_x = map_mat(lambda x: x if x&gt;1e-8 else 1e-8, cel_old_x) loss = crossentropyloss(cel_old_x, targ) total_loss = sum(loss) / len(targ) # print(&quot;Loss is &quot;, loss) # backward pass: # print(shape(cel_old_x), shape(targ)) grad = crossen_b(cel_old_x, targ) # print(grad) # print(shape(grad), grad) grad = softmax_b(cel_old_x,grad) # print(grad) # print(shape(grad), grad) grad, grad_w2, grad_b2 = linear_b(l2,w2,grad) # print(grad, grad_w2[10], grad_b2) # print(shape(grad), grad) grad = map_mat(relu_b,l1,grad) # relu_b needs to use old_x!!! # print(grad) # print(shape(grad), grad) grad, grad_w1, grad_b1 = linear_b(inp,w1,grad) # print(grad, grad_w1[10], grad_b1) # print(shape(grad), grad) return (grad_w1, grad_b1, grad_w2, grad_b2), total_loss, w1, b1, w2, b2 . def make_prediction(inp, w1, b1, w2, b2): inp = reshape(inp, (1, 784)) l1 = linear(inp,w1,b1) l2 = map_mat(relu, l1) sm_old_y = linear(l2,w2,b2) result = softmax(sm_old_y) result = result[0] return result.index(max(result)) # return result . w1 = lst_random((784, 56), True) w2 = lst_random((56, 10), True) b1 = lst_nums(56, 0) b2 = lst_nums(10, 0) wbs = (w1, b1, w2, b2) . Time to train . 6400 // 64 . len(list(filter(None, [make_prediction(py_test_imgs[i], w1, b1, w2, b2) == py_test_labels[i] for i in range(100)]))) . 78 . def train (n, w1, b1, w2, b2, x=x, y=y, bs=64, lr=0.01): &quot;Train n times and return weights and biases&quot; for i in range(n): for j in range(len(x) // bs): xb = x[j*bs:(j+1)*bs] yb = y[j*bs:(j+1)*bs] # Do a forward and backward then get grad # grads = grad_w1, grad_b1, grad_w2, grad_b2 # grad_w1, grad_b1, grad_w2, grad_b2 = forward_and_backward(xb, yb) grads, loss, w1, b1, w2, b2 = forward_and_backward(xb, yb, w1, b1, w2, b2) # print(f&#39;BEFORE = w1: {w1[0]}, b1: {shape(b1)}, w2: {shape(w2)}, b2: {shape(b2)}&#39;) # multiply grads with lr grads = [map_mat(lambda x: x*lr, mat) for mat in grads] w1 = map_mat(op.sub, w1, grads[0]) b1 = map_mat(op.sub, b1, grads[1]) w2 = map_mat(op.sub, w2, grads[2]) b2 = map_mat(op.sub, b2, grads[3]) if j % 50 == 0: # preds = [make_prediction(py_test_imgs[i]) == py_test_labels[i] for i in range(100)] # accuracy = len(list(filter(None, preds))) accuracy = len(list(filter(None, [make_prediction(py_test_imgs[i], w1, b1, w2, b2) == py_test_labels[i] for i in range(100)]))) print(f&quot; Batch #{j} with Loss is {loss}, Accuracy is {accuracy}%&quot;) print(f&quot;Epoch:{i+1} / {n} Loss is {loss}, Accuracy is {accuracy}%&quot;) return (w1, b1, w2, b2) . w1, b1, w2, b2 = train(2, w1, b1, w2, b2, lr=0.01) . NameError Traceback (most recent call last) &lt;ipython-input-181-0b1778a7ea3a&gt; in &lt;module&gt;() -&gt; 1 w1, b1, w2, b2 = train(2, w1, b1, w2, b2, lr=0.01) &lt;ipython-input-178-a5f2ae5806be&gt; in train(n, w1, b1, w2, b2, x, y, bs, lr) 8 # grads = grad_w1, grad_b1, grad_w2, grad_b2 9 # grad_w1, grad_b1, grad_w2, grad_b2 = forward_and_backward(xb, yb) &gt; 10 grads, loss, w1, b1, w2, b2 = forward_and_backward(xb, yb, w1, b1, w2, b2) 11 # print(f&#39;BEFORE = w1: {w1[0]}, b1: {shape(b1)}, w2: {shape(w2)}, b2: {shape(b2)}&#39;) 12 # multiply grads with lr &lt;ipython-input-180-83c41e9ecc5d&gt; in forward_and_backward(inp, targ, w1, b1, w2, b2) 25 # print(grad, grad_w2[10], grad_b2) 26 # print(shape(grad), grad) &gt; 27 grad = map_mat(relu_b,l1,grad) # relu_b needs to use old_x!!! 28 # print(grad) 29 # print(shape(grad), grad) &lt;ipython-input-14-ce1f918df250&gt; in map_mat(fn, mat1, mat2) 20 return list(map(fn, mat1, mat2)) 21 elif (m1r, m1c) == (m2r, m2c): # two matrixs with same sizes &gt; 22 return [[fn(x,y) for x,y in zip(mat1[i], mat2[i])] for i in range(len(mat1))] 23 elif m1c == m2r and m2c==0: # shape of (a, b), (b,) 24 for i in range(m1r): &lt;ipython-input-14-ce1f918df250&gt; in &lt;listcomp&gt;(.0) 20 return list(map(fn, mat1, mat2)) 21 elif (m1r, m1c) == (m2r, m2c): # two matrixs with same sizes &gt; 22 return [[fn(x,y) for x,y in zip(mat1[i], mat2[i])] for i in range(len(mat1))] 23 elif m1c == m2r and m2c==0: # shape of (a, b), (b,) 24 for i in range(m1r): &lt;ipython-input-14-ce1f918df250&gt; in &lt;listcomp&gt;(.0) 20 return list(map(fn, mat1, mat2)) 21 elif (m1r, m1c) == (m2r, m2c): # two matrixs with same sizes &gt; 22 return [[fn(x,y) for x,y in zip(mat1[i], mat2[i])] for i in range(len(mat1))] 23 elif m1c == m2r and m2c==0: # shape of (a, b), (b,) 24 for i in range(m1r): &lt;ipython-input-77-a7b2d6c94954&gt; in relu_b(old_x, grad) 1 def relu_b (old_x, grad): -&gt; 2 return grad if old_ &gt; 0 else 0 3 # return np.where(self.old_x&gt;0,grad,0) NameError: name &#39;old_&#39; is not defined . Although it runs, it takes forever. Now we try to build it with classes, which is more pythonic way than functional approach. . make_prediction(py_imgs[3], w1, b1, w2, b2), py_train_labels[3] . (8, 1) . statistics.mean(w1[0]) . 0.005372460628825117 . w2[0] . [-0.06007763975570099, -1.2259198133278444, -0.09897222247527464, -0.3071114102461263, -0.27948984458414744, -0.17808917189109189, -0.7047370176018776, 0.030757045422782184, -0.24436953598084943, -0.24647854014061663] .",
            "url": "https://meinkappa.github.io/blog/2021/09/03/My-First-Neural-Net.html",
            "relUrl": "/2021/09/03/My-First-Neural-Net.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "First markdown page",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Strikethrough . Strike me hard!! . Taskings to complete . Wake up. | Try to stay awake. | Try harder. | Give it a little more. | Maybe a tiny bit more. | Take a little break. | . Footnotes . This is the footnote. &#8617; . |",
            "url": "https://meinkappa.github.io/blog/markdown/2021/08/01/test-post.html",
            "relUrl": "/markdown/2021/08/01/test-post.html",
            "date": " • Aug 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://meinkappa.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://meinkappa.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://meinkappa.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://meinkappa.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}